---
title: "Program"
bg: gainsboro
color: black
fa-icon: tasks
---

## Technical Program 

All times refer to Friday, Nov. 17, 2017

<table>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 8:30am - 8:35am </td>
<td style="padding: 10px; border-bottom: 1px solid; text-align: center;"> ---- Welcome and Introduction ---- </td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 8:35am - 9:35am </td>
<td style="padding: 10px; border-bottom: 1px solid;"> 
<details>
  <summary>Lucy Nowell, PhD, Program Manager, Department of Energy Advanced Scientific Computing Research</summary>
  <div style="padding: 10px;"><img src="../img/Nowell_Nov2015.jpg" style="float: left; padding-right: 10px;">
  Dr. Lucy Nowell is a Computer Scientist and Program Manager in the Advanced Scientific Computing Research (ASCR) program in the Department of Energy’s Office of Science. Until recently, she managed a research portfolio emphasizing scientific data management, analysis and visualization. A change of assignment has her now focused on reshaping the ASCR Computer Science program to address challenges in the realm of operating and runtime systems and programming models/environments that will result from exponential increases in the complexity of Post Moore Era supercomputers. Previously she served as a Program Director in NSF’s Office of Cyberinfrastructure and as a Program Manager for the Department of Defense, managing projects related to information analysis and visualization. Her MS and PhD in Computer Science are from Virginia Tech. She has a BA and MA in Theatre from the University of Alabama – Tuscaloosa and the MFA from the University of New Orleans. Her own research focused on information visualization for digital libraries and science applications, drawing on her background in visual art and cognitive/perceptual psychology, as well as computer science.</div>
</details>

<details>
  <summary>Keynote Talk: Visual Performance Analysis for Extremely Heterogeneous Systems</summary>
  <div style="padding: 10px;"><em>Extreme heterogeneity</em> is the result of using multiple types of processors, accelerators, memory and storage in a single computing platform or environment that must support an expanding variety of application workflows to meet the needs of increasingly heterogeneous users. Extremely heterogeneous supercomputers are likely be acquired by the ASCR-supported supercomputing facilities as we reach the end of Moore’s Law while still facing rapidly increasing computational and data intensive requirements. The exponential increase in system complexity will make it essential for system administrators and software developers to have new tools that help them understand the behavior of extremely heterogeneous supercomputing environments and the applications that run in them. The vast bandwidth of visual perception makes the combination of visualization and performance analysis essential.</div>
</details>
</td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 9:35am - 10:00am </td>
<td style="padding: 10px; border-bottom: 1px solid;"> 
<details>
<summary>Paper Talk: Nico Reissmann, Magnus Jahre and <strong>Ananya Muddukrishna</strong>. <a href="pdfs/VPA_2017_reissman.pdf">Towards Aggregated Grain Graphs</a></summary>
<div style="padding: 10px;">Grain graphs simplify OpenMP performance analysis by visualizing performance problems from a fork-join perspective that is familiar to programmers. However, it is tedious to navigate and diagnose problems in large grain graphs with thousands of task and parallel for-loop chunk instances. We present an aggregation method that matches recurring patterns in grain graphs and groups related nodes together, reducing graphs of any size to one root group. The aggregated grain graph is then navigated by progressively uncovering groups and analyzing only those groups that have problems. This enhances productivity by enabling programmers to understand program structure and problems in large grain graphs with less effort than before.  <a href="pdfs/VPA_2017_reissman.pdf">[PDF]</a></div>
</details>
</td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 10:00am - 10:30am </td>
<td style="padding: 10px; border-bottom: 1px solid; text-align: center"> ---- Coffee Break ---- </td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 10:30am - 11:10am </td>
<td style="padding: 10px; border-bottom: 1px solid;"> 
<details>
<summary>Panel Discussion: Challenges and the Future of HPC Performance Visualization</summary>
<div style="padding: 10px;">
Panelists:<br/>
<ul style="font-size: 100%;">
<li>Holger Brunst, TU Dresden</li>
<li>Katherine Isaacs, University of Arizona</li>
<li>David Richards, Lawrence Livermore National Laboratory</li>
</ul>
</div>
</details>
</td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 11:10am - 11:35am </td>
<td style="padding: 10px; border-bottom: 1px solid;"> 
<details>
<summary>Paper Talk: <strong>Chad Wood</strong>, Matthew Larsen, Alfredo Gimenez, Cyrus Harrison, Todd Gamblin and Allen Malony. <a href="pdfs/VPA_2017_wood.pdf">Projecting Performance Data Over Simulation Geometry Using SOSflow and Alpine</a></summary>
<div style="padding: 10px;">The performance of HPC simulation codes is often tied to their simulated domains; e.g., properties of the input decks, boundaries of the underlying meshes, and parallel decomposition of the simulation space. A variety of research efforts have demonstrated the utility of projecting performance data onto the simulation geometry to enable analysis of these kinds of performance problems. However, current methods to do so are largely ad-hoc and limited in terms of extensibility and scalability. Furthermore, few methods enable this projection online, resulting in large storage and processing requirements for offline analysis. We present a general, extensible, and scalable solution for in-situ (online) visualization of performance data projected onto the underlying geometry of simulation codes. Our solution employs the scalable observation system SOSflow with the in-situ visualization framework ALPINE to automatically extract simulation geometry and stream aggregated performance metrics to respective locations within the geometry at runtime. Our system decouples the resources and mechanisms to collect, aggregate, project, and visualize the resulting data, thus mitigating overhead and enabling online analysis at large scales. Furthermore, our method requires minimal user input and modification of existing code, enabling general and widespread adoption.  <a href="pdfs/VPA_2017_wood.pdf">[PDF]</a></div>
</details>
</td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid; border-bottom: 1px solid;"> 11:35am - 12:00pm </td>
<td style="padding: 10px; border-bottom: 1px solid;">
<details>
<summary>Paper Talk: <strong>Matthias Diener</strong>, Sam White and Laxmikant Kale. <a href="pdfs/VPA_2017_diener.pdf">Visualizing, measuring, and tuning Adaptive MPI parameters</a></summary>
<div style="padding: 10px;">Adaptive MPI (AMPI) is an advanced MPI runtime environment that offers several features over traditional MPI runtimes, which can lead to a better utilization of the underlying hardware platform and therefore higher performance. These features are overdecomposition through virtualization, and load balancing via rank migration. Choosing which of these features to use, and finding the optimal parameters for them is a challenging task however, since different applications and systems may require different options. Furthermore, there is a lack of information about the impact of each option. In this paper, we present a new visualization of AMPI in its companion Projections tool, which depicts the operation of an MPI application and details the impact of the different AMPI features on its resource usage. We show how these visualizations can help to improve the efficiency and execution time of an MPI application. Applying optimizations indicated by the performance analysis to two MPI-based applications results in performance improvements of up 18% from overdecomposition and load balancing.  <a href="pdfs/VPA_2017_diener.pdf">[PDF]</a></div>
</details>
</td>
</tr>
<tr>
<td style="padding: 5px; width: 140px; border-right: 1px solid;">  </td>
<td style="padding: 10px; text-align: center;"> ---- Closing ---- </td>
</tr>
</table>
